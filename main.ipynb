{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"OHEdoiIqwdlq"},"outputs":[],"source":["import torch\n","\n","!pip uninstall torch-scatter torch-sparse torch-geometric torch-cluster  --y\n","!pip install torch-scatter -f https://data.pyg.org/whl/torch-{torch.__version__}.html\n","!pip install torch-sparse -f https://data.pyg.org/whl/torch-{torch.__version__}.html\n","!pip install torch-cluster -f https://data.pyg.org/whl/torch-{torch.__version__}.html\n","!pip install git+https://github.com/pyg-team/pytorch_geometric.git\n","!pip install torch_geometric\n","!pip install sklearn"]},{"cell_type":"code","source":["import pandas as pd\n","import requests\n","from time import sleep\n","import os\n","import json\n","from geopy.distance import geodesic\n","import sys\n","import pickle\n","import numpy as np\n","import torch_geometric\n","import torch_geometric.nn as nn\n","import torch_geometric.utils as utils\n","import torch_geometric.transforms as T\n","from torch_geometric.data import hetero_data\n","import torch\n","from sklearn.preprocessing import MultiLabelBinarizer\n","\n","import numpy as np\n","import torch\n","from torch_geometric.data import HeteroData\n","import torch_geometric.transforms as T\n","from torch_geometric.nn import Sequential, Linear\n","from torch.nn import ReLU\n","from torch_geometric.datasets import OGB_MAG\n","from torch_geometric.nn import SAGEConv, to_hetero\n","from torch_geometric.nn.conv import HeteroConv, SAGEConv, GATConv, GraphConv\n","from torch_geometric.loader import NeighborLoader, HGTLoader\n","import torch.nn.functional as F\n","import torch_sparse"],"metadata":{"id":"_fKgc7xVzu0T","executionInfo":{"status":"ok","timestamp":1701819208191,"user_tz":360,"elapsed":4195,"user":{"displayName":"Pietro Lodi Rizzini","userId":"06744866208336726389"}}},"execution_count":2,"outputs":[]},{"cell_type":"code","source":["from google.colab import drive\n","\n","# Mount Google Drive\n","drive.mount('/content/drive')\n","\n","folder_path = '/content/drive/MyDrive/ml_proj/'\n","\n","data_path = folder_path + 'data/'"],"metadata":{"id":"HqJC5uDKz0KF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["listings = pd.read_csv(folder_path + 'data/listings_clean.csv')\n","cta = pd.read_csv(folder_path + 'data/cta.csv')\n","pois = pd.read_csv(folder_path + 'data/pois.csv')"],"metadata":{"id":"GrkBEiO6z00f","executionInfo":{"status":"ok","timestamp":1701819238935,"user_tz":360,"elapsed":2795,"user":{"displayName":"Pietro Lodi Rizzini","userId":"06744866208336726389"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["listings_price = listings['price']\n","listings_features = listings.drop(columns=['price'])"],"metadata":{"id":"12njEDvE4rFZ","executionInfo":{"status":"ok","timestamp":1701819239267,"user_tz":360,"elapsed":4,"user":{"displayName":"Pietro Lodi Rizzini","userId":"06744866208336726389"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","source":["poi_edge_index = [[], []]\n","poi_edge_weights = []\n","\n","\n","if os.path.exists(folder_path + 'data/poi_edges_dist.pkl'):\n","    with open(folder_path + 'data/poi_edges_dist.pkl', 'rb') as file:\n","        (poi_edge_index, poi_edge_weights) = pickle.load(file)\n","else:\n","    for listing_id, listing in listings.iterrows():\n","        listing_coord = listing[['latitude', 'longitude']].values * (np.pi / 180) * 6371000\n","\n","        poi_coords = pois[['lat', 'lon']].values * (np.pi / 180) * 6371000\n","\n","        distances = np.linalg.norm((listing_coord - poi_coords).astype(int), axis=1)\n","        weights = 1 / (distances + .01)\n","        #weights = distances\n","\n","        # d < thresh => 1/d > 1/thresh\n","        thresh = 50\n","\n","        close_pois = np.where(distances < thresh)[0]\n","\n","        if close_pois.size > 0:\n","            poi_edge_index[0].extend([listing_id] * close_pois.size)\n","            poi_edge_index[1].extend(close_pois)\n","            poi_edge_weights.extend(weights[close_pois])\n","\n","        if listing_id % 1000 == 0:\n","            print(f\"{listing_id}/{len(listings)}\")\n","\n","    with open(folder_path + 'data/poi_edges_dist.pkl', 'wb') as file:\n","        pickle.dump((poi_edge_index, poi_edge_weights), file)\n","\n","print(len(poi_edge_index[0]))\n","print(len(poi_edge_index[1]))\n","print(len(poi_edge_weights))\n","print(poi_edge_index[0][:20])\n","print(poi_edge_index[1][:20])\n","print(poi_edge_weights[:20])"],"metadata":{"id":"EmXtWtEB5l1Y"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["cta_edge_index = [[], []]\n","cta_edge_weights = []\n","\n","\n","if os.path.exists(folder_path + 'data/cta_edges_dist.pkl') and False:\n","    with open(folder_path + 'data/cta_edges_dist.pkl', 'rb') as file:\n","        (cta_edge_index, cta_edge_weights) = pickle.load(file)\n","else:\n","    for listing_id, listing in listings.iterrows():\n","        listing_coord = listing[['latitude', 'longitude']].values * (np.pi / 180) * 6371000\n","\n","        cta_coords = cta[['Latitude', 'Longitude']].values * (np.pi / 180) * 6371000\n","\n","        distances = np.linalg.norm((listing_coord - cta_coords).astype(int), axis=1)\n","        weights = 1/distances\n","\n","        thresh = 300\n","\n","        close_cta = np.where(weights >1/ thresh)[0]\n","\n","        if close_cta.size > 0:\n","            cta_edge_index[0].extend([listing_id] * close_cta.size)\n","            cta_edge_index[1].extend(close_cta)\n","            cta_edge_weights.extend(weights[close_cta])\n","\n","\n","        if listing_id % 1000 == 0:\n","            print(f\"{listing_id}/{len(listings)}\")\n","\n","    with open(folder_path + 'data/cta_edges_dist.pkl', 'wb') as file:\n","        pickle.dump((cta_edge_index, cta_edge_weights), file)\n","\n","print(len(cta_edge_index[0]))\n","print(len(cta_edge_index[1]))\n","print(len(cta_edge_weights))\n","print(cta_edge_index[0][:20])\n","print(cta_edge_index[1][:20])\n","print(cta_edge_weights[:20])"],"metadata":{"id":"pslxlISOwlqD"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["listings_edge_index = [[], []]\n","listings_edge_weights = []\n","\n","with open(folder_path + 'data/doc_inference.pkl', 'rb') as file:\n","        doc_inference = pickle.load(file)\n","\n","doc_inference_t = torch.tensor(doc_inference)\n","\n","if os.path.exists(folder_path + 'data/listings_edges.pkl') and False:\n","    with open(folder_path + 'data/listings_edges.pkl', 'rb') as file:\n","        (listings_edge_index, listings_edge_weights) = pickle.load(file)\n","else:\n","    for listing_id, listing in listings.iterrows():\n","        listing_coord = listing[['latitude', 'longitude']].values * (np.pi / 180) * 6371000\n","\n","        embedding_i = doc_inference[listing_id]\n","\n","        embedding_i_expanded = doc_inference_t[listing_id].unsqueeze(0).expand_as(doc_inference_t)\n","\n","        cosine_similarity = F.cosine_similarity(embedding_i_expanded, doc_inference_t, dim=1)\n","        cosine_similarity[listing_id] = 0\n","\n","\n","        listings_coord = listings[['latitude', 'longitude']].values * (np.pi / 180) * 6371000\n","\n","        distances = np.linalg.norm((listing_coord - listings_coord).astype(int), axis=1)\n","        distances[listing_id] = 99999\n","        weights = 1 / (distances + .01)\n","\n","        # d < thresh => 1/d > 1/thresh\n","        thresh = 100\n","        cosine_sim_thresh = .3\n","\n","        sim_listings = np.where(torch.tensor(distances < thresh) & (cosine_similarity > cosine_sim_thresh))[0]\n","\n","        if sim_listings.size > 0:\n","            listings_edge_index[0].extend([listing_id] * sim_listings.size)\n","            listings_edge_index[1].extend(sim_listings)\n","            listings_edge_weights.extend(weights[sim_listings])\n","\n","        if listing_id % 1000 == 0:\n","            print(f\"{listing_id}/{len(listings)}\")\n","\n","    #with open(folder_path + 'data/listings_edges.pkl', 'wb') as file:\n","        #pickle.dump((listings_edge_index, listings_edge_weights), file)\n","\n","print(len(listings_edge_index[0]))\n","print(len(listings_edge_index[1]))\n","print(len(listings_edge_weights))\n","print(listings_edge_index[0][:20])\n","print(listings_edge_index[1][:20])\n","print(listings_edge_weights[:20])"],"metadata":{"id":"9hI6jA_67ocy"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from sklearn.preprocessing import StandardScaler\n","\n","data = HeteroData()\n","\n","scaler = StandardScaler()\n","listings_scaled = scaler.fit_transform(listings.to_numpy())\n","pois_scaled = scaler.fit_transform(pois[['price', 'type']].to_numpy())\n","\n","data['listing'].x = torch.tensor(listings_scaled, dtype=torch.float32)\n","data['listing'].y = torch.tensor(np.log(listings_price.to_numpy()), dtype=torch.float32)\n","data['poi'].x = torch.tensor(pois_scaled, dtype=torch.float32)\n","data['listing', 'is_near', 'poi'].edge_index = torch.tensor(poi_edge_index, dtype=torch.int64)\n","data['listing', 'is_near', 'poi'].edge_weight = torch.tensor(poi_edge_weights, dtype=torch.float32)\n","\n","data"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9qbXorCd0p7y","outputId":"4c34a994-d59e-4237-af32-bd0ca7251f7f","executionInfo":{"status":"ok","timestamp":1701819706802,"user_tz":360,"elapsed":350,"user":{"displayName":"Pietro Lodi Rizzini","userId":"06744866208336726389"}}},"execution_count":23,"outputs":[{"output_type":"execute_result","data":{"text/plain":["HeteroData(\n","  listing={\n","    x=[8528, 13],\n","    y=[8528],\n","  },\n","  poi={ x=[3157, 2] },\n","  (listing, is_near, poi)={\n","    edge_index=[2, 1870],\n","    edge_weight=[1870],\n","  }\n",")"]},"metadata":{},"execution_count":23}]},{"cell_type":"code","source":["data = T.RandomNodeSplit()(data)\n","data = T.ToUndirected()(data)\n","data"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"QzbF_LwyxI_x","outputId":"d587c956-0eea-4810-d017-819b4a0182be","executionInfo":{"status":"ok","timestamp":1701819708862,"user_tz":360,"elapsed":351,"user":{"displayName":"Pietro Lodi Rizzini","userId":"06744866208336726389"}}},"execution_count":24,"outputs":[{"output_type":"execute_result","data":{"text/plain":["HeteroData(\n","  listing={\n","    x=[8528, 13],\n","    y=[8528],\n","    train_mask=[8528],\n","    val_mask=[8528],\n","    test_mask=[8528],\n","  },\n","  poi={ x=[3157, 2] },\n","  (listing, is_near, poi)={\n","    edge_index=[2, 1870],\n","    edge_weight=[1870],\n","  },\n","  (poi, rev_is_near, listing)={\n","    edge_index=[2, 1870],\n","    edge_weight=[1870],\n","  }\n",")"]},"metadata":{},"execution_count":24}]},{"cell_type":"code","source":["import sklearn\n","from sklearn.metrics import mean_squared_error\n","import matplotlib.pyplot as plt\n","\n","\n","\n","class HeteroGNN(torch.nn.Module):\n","  def __init__(self, metadata, hidden_channels, out_channels, num_layers):\n","    super().__init__()\n","    self.convs = torch.nn.ModuleList()\n","\n","    for _ in range(num_layers):\n","        conv = HeteroConv({edge_type: torch_geometric.nn.conv.GraphConv((-1, -1), hidden_channels) for edge_type in data.edge_types}, aggr='sum')\n","        self.convs.append(conv)\n","\n","\n","    self.lin = Linear(hidden_channels, out_channels)\n","\n","  def forward(self, x_dict, edge_index_dict):\n","      for conv in self.convs:\n","        x_dict = conv(x_dict, edge_index_dict, edge_weight_dict=data.edge_weight_dict)\n","        x_dict = {key: F.leaky_relu(x) for key, x in x_dict.items()}\n","\n","      return self.lin(x_dict['listing']).clamp(1,8)\n","\n","model = HeteroGNN(data.metadata(), hidden_channels=1024, out_channels=1, num_layers=2)\n","optimizer = torch.optim.Adam(model.parameters())\n","\n","def train():\n","  model.train()\n","  optimizer.zero_grad()\n","  out = model(data.x_dict, data.edge_index_dict)\n","  mask = data['listing'].train_mask\n","  target = data['listing'].y[mask].unsqueeze(1)\n","\n","  loss = F.mse_loss(out[mask], target)\n","  loss.backward()\n","  optimizer.step()\n","  return float(loss)\n","\n","@torch.no_grad()\n","def test():\n","  model.eval()\n","  pred = model(data.x_dict, data.edge_index_dict)\n","\n","  accs = []\n","  for split in ['train_mask', 'val_mask', 'test_mask']:\n","      mask = data['listing'][split]\n","      acc = sklearn.metrics.mean_squared_error(data['listing'].y[mask], pred[mask])\n","      accs.append(float(acc))\n","      acc = sklearn.metrics.mean_absolute_error(data['listing'].y[mask], pred[mask])\n","      accs.append(float(acc))\n","  return accs\n","\n","epochs_list = []\n","train_mse_list = []\n","val_mse_list = []\n","test_mse_list = []\n","train_mae_list = []\n","val_mae_list = []\n","test_mae_list = []\n","\n","\n","for epoch in range(1, 100000000):\n","  loss = train()\n","  train_mse, train_mae, val_mse, val_mae, test_mse, test_mae = test()\n","\n","  epochs_list.append(epoch)\n","  train_mse_list.append(train_mse)\n","  val_mse_list.append(val_mse)\n","  test_mse_list.append(test_mse)\n","\n","  if epoch % 1 == 0:\n","    plt.plot(epochs_list, train_mse_list , label='Train Loss')\n","    plt.plot(epochs_list, val_mse_list, label='Test Loss')\n","\n","    plt.xlabel('Epochs')\n","    plt.ylabel('Loss')\n","    plt.title('Training / Test Losses')\n","    plt.legend()\n","    plt.show()\n","\n","\n","  print(f'Epoch: {epoch:03d}, Loss: {loss:.4f}, \\n mse  Error: Train: {train_mse:.4f}, ' f'Val: {val_mse:.4f}, Test: {test_mse:.4f}')\n","  print(f'Epoch: {epoch:03d}, Loss: {loss:.4f}, \\n mae  Error: Train: {train_mae:.4f}, ' f'Val: {val_mae:.4f}, Test: {test_mae:.4f}')\n","\n","\n"],"metadata":{"id":"L1htfZPPwvYx"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import os.path as osp\n","import time\n","\n","import torch\n","import torch.nn.functional as F\n","from sklearn.linear_model import LinearRegression\n","\n","import torch_geometric.transforms as T\n","from torch_geometric.datasets import Planetoid\n","from torch_geometric.loader import LinkNeighborLoader\n","from torch_geometric.nn import GraphSAGE\n","from torch_geometric.data import Data\n","from sklearn.metrics import mean_squared_error\n","\n","\n","scaler = StandardScaler()\n","listings_scaled = scaler.fit_transform(listings.to_numpy())\n","\n","\n","data = Data(x=torch.tensor(listings_scaled, dtype=torch.float32),\n","            edge_index=torch.tensor(listings_edge_index, dtype=torch.int64),\n","            edge_weight=torch.tensor(listings_edge_weights, dtype=torch.float32),\n","            y=torch.tensor(np.log(listings_price.to_numpy()), dtype=torch.float32)\n",")\n","data = T.RandomNodeSplit()(data)\n","\n","print(data)\n","\n","train_loader = LinkNeighborLoader(\n","    data,\n","    batch_size=256,\n","    shuffle=True,\n","    neg_sampling_ratio=1.0,\n","    num_neighbors=[10, 10],\n",")\n","\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","data = data.to(device, 'x', 'edge_index')\n","print(device)\n","\n","model = GraphSAGE(\n","    data.num_node_features,\n","    hidden_channels=64,\n","    num_layers=2,\n",").to(device)\n","\n","optimizer = torch.optim.Adam(model.parameters())\n","\n","\n","def train():\n","    model.train()\n","\n","    total_loss = 0\n","    for batch in train_loader:\n","        batch = batch.to(device)\n","        optimizer.zero_grad()\n","        h = model(batch.x, batch.edge_index)\n","        h_src = h[batch.edge_label_index[0]]\n","        h_dst = h[batch.edge_label_index[1]]\n","        pred = (h_src * h_dst).sum(dim=-1)\n","        loss = F.binary_cross_entropy_with_logits(pred, batch.edge_label)\n","        loss.backward()\n","        optimizer.step()\n","        total_loss += float(loss) * pred.size(0)\n","\n","    return total_loss / data.num_nodes\n","\n","\n","\n","@torch.no_grad()\n","def test():\n","    model.eval()\n","    out = model(data.x, data.edge_index, edge_weight=data.edge_weight).cpu()\n","\n","    print(out.shape)\n","\n","    clf = LinearRegression()\n","    clf.fit(out[data.train_mask], data.y[data.train_mask])\n","\n","    val_acc = mean_squared_error(clf.predict(out[data.val_mask]), data.y[data.val_mask])\n","    test_acc = mean_squared_error(clf.predict(out[data.test_mask]), data.y[data.test_mask])\n","\n","    return val_acc, test_acc\n","\n","\n","for epoch in range(1, 1000):\n","    loss = train()\n","    val_acc, test_acc = test()\n","    print(f'Epoch: {epoch:03d}, Loss: {loss:.4f}, '\n","          f'Val: {val_acc:.4f}, Test: {test_acc:.4f}')\n"],"metadata":{"id":"QI7OXuvMlSBV"},"execution_count":null,"outputs":[]}]}